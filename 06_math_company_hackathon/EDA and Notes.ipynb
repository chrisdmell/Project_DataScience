{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Heading\n",
    "#### Sub heading\n",
    "##### Notes\n",
    "> - Important points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Follow this process](https://www.kaggle.com/shweta2407/house-prices-regression-predictive-analysis)<br>\n",
    "[Flask app](https://www.youtube.com/watch?v=DVxkI1VmpCk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 001 Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root mean squared log error(RMSLE)<br>\n",
    "When to use RMSLE - When we want to penalize the under predictions and over predictions are fine. This case arises when we the model isnt bounded usually in XGBoost or Catboost. in GMLnet where the model is bounded we do not need these metrics.<br>\n",
    "\n",
    "RMSLE - we can see that for under predictions RMLSE gives higher number as compared to over predictions. <br>\n",
    "[credits](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img\\rmsle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img\\rmse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img\\rmsle_for.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(real, predicted):\n",
    "    sum=0.0\n",
    "    for x in range(len(predicted)):\n",
    "        if predicted[x]<1 or real[x]<1: #check for negative values\n",
    "            ## negative and 0 values give us nan and -inf results \n",
    "            continue\n",
    "        p = np.log(predicted[x]+1)\n",
    "        r = np.log(real[x]+1)\n",
    "        sum = sum + (p - r)**2\n",
    "    return (sum/len(predicted))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.411728991487363"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12231.000000000004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8da60eb1f1f2>:7: RuntimeWarning: invalid value encountered in log\n",
      "  display(np.log(-100))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8da60eb1f1f2>:9: RuntimeWarning: divide by zero encountered in log\n",
      "  display(np.log(00))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8da60eb1f1f2>:11: RuntimeWarning: invalid value encountered in log\n",
      "  display(np.log(-1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "display(np.log(12231))\n",
    "\n",
    "display(np.exp(9.411728991487363))\n",
    "\n",
    "display(np.log(-100))\n",
    "\n",
    "display(np.log(00))\n",
    "\n",
    "display(np.log(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03973012298459379"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "y_true = [3, 5, 2.5, 7]\n",
    "y_pred = [2.5, 5, 4, 8]\n",
    "mean_squared_log_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19932416558108"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 002 About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There seems to be no null values in the data.\n",
    "2. 'ID', 'Price', 'Levy', 'Manufacturer', 'Model', 'Prod. year',\n",
    "       'Category', 'Leather interior', 'Fuel type', 'Engine volume', 'Mileage',\n",
    "       'Cylinders', 'Gear box type', 'Drive wheels', 'Doors', 'Wheel', 'Color',\n",
    "       'Airbags'  are the columns in the dataset\n",
    "4. We might need to label encode a few columns such as __Doors__\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Levy is tax, and all cars have taxes, might have to impute averages or something here. Makes sense to remove this column.\n",
    "2. Price - this starts from 1 to 10^7. So we might need to treat these values as well. At around $10K the prices seem right. \n",
    "3. Price and production year should be most or less highly correlated but thats not to be seen here. The corr is 0.01 which isnt great.\n",
    "4. For prices less than 25% quantile we can tag them. \n",
    "5. Manufacturer\tModel\t,Prod. year,\tCategory\t,Leather interior,\tFuel type are good to go. no issues\n",
    "6. Engine volume has 0 values. Makes sense to remove them. \n",
    "7. Engine Volume can be made into 2 columns with and without Trubo for next models\n",
    "8. Mileage - impute 0 with average for now\n",
    "9. Doors - 2 and 4. Can be cleaned into int. For now lets keep it that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Manufacturer',\n",
       " 'Model',\n",
       " 'Prod. year',\n",
       " 'Category',\n",
       " 'Leather interior',\n",
       " 'Fuel type',\n",
       " 'Cylinders',\n",
       " 'Gear box type',\n",
       " 'Drive wheels',\n",
       " 'Doors',\n",
       " 'Wheel',\n",
       " 'Airbags']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"Manufacturer\",\"Model\",\"Prod. year\",\"Category\",\"Leather interior\",\"Fuel type\", \"Cylinders\",\"Gear box type\", \\\n",
    " \"Drive wheels\", \"Doors\", \"Wheel\", \"Airbags\"]\n",
    "## There are my initial baseline model and this is clean more or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 003 Baseline model 001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 10495.339442139302,\n",
       " 'mse': 846828365.6053432,\n",
       " 'rmse': 29100.315558518316,\n",
       " 'r2': -1.7350164169889943}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'mae': 10495.339442139302,\n",
    " 'mse': 846828365.6053432,\n",
    " 'rmse': 29100.315558518316,\n",
    " 'r2': -1.7350164169889943}\n",
    "\n",
    "## model first used 001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__R square can have a negative value when the model selected does not follow the trend of the data, therefore leading to a worse fit than the horizontal line. It is usually the case when there are constraints on either the intercept or the slope of the linear regression line.__ [1](https://stats.stackexchange.com/questions/12900/when-is-r-squared-negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img\\r2_negative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[What’s the Difference Between RMSE and RMSLE?](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other model metrics\n",
    "\n",
    "> Before using ML flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 11003.275172396843,\n",
       " 'mse': 1878913140.0361242,\n",
       " 'rmse': 43346.43168746563,\n",
       " 'r2': -5.068358705039009}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'mae': 11003.275172396843,\n",
    " 'mse': 1878913140.0361242,\n",
    " 'rmse': 43346.43168746563,\n",
    " 'r2': -5.068358705039009}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 13460.874916258063,\n",
       " 'mse': 32758922041.762417,\n",
       " 'rmse': 180994.25969284886,\n",
       " 'r2': -104.80206477027522}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'mae': 13460.874916258063,\n",
    " 'mse': 32758922041.762417,\n",
    " 'rmse': 180994.25969284886,\n",
    " 'r2': -104.80206477027522}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 8783.26612354863,\n",
       " 'mse': 680056489.2811434,\n",
       " 'rmse': 26077.892730838958,\n",
       " 'r2': -1.1963903645743579}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'mae': 8783.26612354863,\n",
    " 'mse': 680056489.2811434,\n",
    " 'rmse': 26077.892730838958,\n",
    " 'r2': -1.1963903645743579}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'mae': 0.3127371392570263,\n",
    " 'mse': 0.239841862218088,\n",
    " 'rmse': 0.48973652326336453,\n",
    " 'r2': 0.47416038720583453}\n",
    "\n",
    "##  Using log(label). Under predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'mae': 0.7161122603509764,\n",
    " 'mse': 1.2538596551355954,\n",
    " 'rmse': 1.1197587486309697,\n",
    " 'r2': 0.47850914167938985}\n",
    "\n",
    "##  Using log(label+1) gives good better predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log predictions, gave under predicted values. R2 is positive but the model predictions are low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material to deal with negative predictions\n",
    "\n",
    "1. log(1+x) - I tried just log(x) log(0), this gives nan/inf  values.\n",
    "2. It is an unbounded problem so its common to see such [values](https://github.com/dmlc/xgboost/issues/1581).[1](https://datascience.stackexchange.com/questions/77234/can-boosted-trees-predict-below-the-minimum-value-of-the-training-label/77272#77272).  can I have a predicted value bigger than the maximum value in train? In decision trees - No, In random forest - No,For Gradient boosting - Yes, For Linear Models - Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 004 [catboost parameters](https://effectiveml.com/using-grid-search-to-optimise-catboost-parameters.html)<br>\n",
    "[catboost docs](https://catboost.ai/docs/concepts/python-reference_parameters-list.html)\n",
    "\n",
    "iterations=500\tThe maximum number of trees that can be built when solving machine learning problems. Fewer may be used.\n",
    "\n",
    "learning_rate=0.03\tused for reducing the gradient step. It affects the overall time of training: the smaller the value, the more iterations are required for training.\n",
    "\n",
    "depth=6\tDepth of the tree. Can be any integer up to 32. Good values in the range 1 - 10.\n",
    "\n",
    "l2_leaf_reg=3\ttry different values for the regularizer to find the best possible. Any positive values are allowed. Higher values lead to under-fitting and counter overfit.\n",
    "\n",
    "loss_function='Logloss'\tFor 2-class classification use 'LogLoss' or 'CrossEntropy'. For multiclass use 'MultiClass'.\n",
    "\n",
    "border_count=32\tThe number of splits for numerical features. Allowed values are integers from 1 to 255 inclusively.\n",
    "\n",
    "ctr_border_count=50\tThe number of splits for categorical features. Allowed values are integers from 1 to 255 inclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan_mode': 'Min',\n",
       " 'eval_metric': 'RMSE',\n",
       " 'combinations_ctr': ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1',\n",
       "  'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1'],\n",
       " 'iterations': 100,\n",
       " 'sampling_frequency': 'PerTree',\n",
       " 'fold_permutation_block': 0,\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'counter_calc_method': 'SkipTest',\n",
       " 'grow_policy': 'SymmetricTree',\n",
       " 'penalties_coefficient': 1,\n",
       " 'boosting_type': 'Plain',\n",
       " 'model_shrink_mode': 'Constant',\n",
       " 'feature_border_type': 'GreedyLogSum',\n",
       " 'ctr_leaf_count_limit': 18446744073709551615,\n",
       " 'bayesian_matrix_reg': 0.10000000149011612,\n",
       " 'one_hot_max_size': 2,\n",
       " 'l2_leaf_reg': 3,\n",
       " 'random_strength': 1,\n",
       " 'rsm': 1,\n",
       " 'boost_from_average': True,\n",
       " 'max_ctr_complexity': 1,\n",
       " 'model_size_reg': 0.5,\n",
       " 'simple_ctr': ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1',\n",
       "  'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1'],\n",
       " 'pool_metainfo_options': {'tags': {}},\n",
       " 'subsample': 0.800000011920929,\n",
       " 'use_best_model': False,\n",
       " 'random_seed': 0,\n",
       " 'depth': 9,\n",
       " 'ctr_target_border_count': 1,\n",
       " 'posterior_sampling': False,\n",
       " 'has_time': False,\n",
       " 'store_all_simple_ctr': False,\n",
       " 'border_count': 254,\n",
       " 'classes_count': 0,\n",
       " 'auto_class_weights': 'None',\n",
       " 'sparse_features_conflict_fraction': 0,\n",
       " 'leaf_estimation_backtracking': 'AnyImprovement',\n",
       " 'best_model_min_trees': 1,\n",
       " 'model_shrink_rate': 0,\n",
       " 'min_data_in_leaf': 1,\n",
       " 'loss_function': 'RMSE',\n",
       " 'learning_rate': 0.09000000357627869,\n",
       " 'score_function': 'Cosine',\n",
       " 'task_type': 'CPU',\n",
       " 'leaf_estimation_iterations': 1,\n",
       " 'bootstrap_type': 'MVS',\n",
       " 'max_leaves': 512,\n",
       " 'permutation_count': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'nan_mode': 'Min',\n",
    " 'eval_metric': 'RMSE',\n",
    " 'combinations_ctr': ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1',\n",
    "  'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1'],\n",
    " 'iterations': 100,\n",
    " 'sampling_frequency': 'PerTree',\n",
    " 'fold_permutation_block': 0,\n",
    " 'leaf_estimation_method': 'Newton',\n",
    " 'counter_calc_method': 'SkipTest',\n",
    " 'grow_policy': 'SymmetricTree',\n",
    " 'penalties_coefficient': 1,\n",
    " 'boosting_type': 'Plain',\n",
    " 'model_shrink_mode': 'Constant',\n",
    " 'feature_border_type': 'GreedyLogSum',\n",
    " 'ctr_leaf_count_limit': 18446744073709551615,\n",
    " 'bayesian_matrix_reg': 0.10000000149011612,\n",
    " 'one_hot_max_size': 2,\n",
    " 'l2_leaf_reg': 3,\n",
    " 'random_strength': 1,\n",
    " 'rsm': 1,\n",
    " 'boost_from_average': True,\n",
    " 'max_ctr_complexity': 1,\n",
    " 'model_size_reg': 0.5,\n",
    " 'simple_ctr': ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1',\n",
    "  'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1'],\n",
    " 'pool_metainfo_options': {'tags': {}},\n",
    " 'subsample': 0.800000011920929,\n",
    " 'use_best_model': False,\n",
    " 'random_seed': 0,\n",
    " 'depth': 9,\n",
    " 'ctr_target_border_count': 1,\n",
    " 'posterior_sampling': False,\n",
    " 'has_time': False,\n",
    " 'store_all_simple_ctr': False,\n",
    " 'border_count': 254,\n",
    " 'classes_count': 0,\n",
    " 'auto_class_weights': 'None',\n",
    " 'sparse_features_conflict_fraction': 0,\n",
    " 'leaf_estimation_backtracking': 'AnyImprovement',\n",
    " 'best_model_min_trees': 1,\n",
    " 'model_shrink_rate': 0,\n",
    " 'min_data_in_leaf': 1,\n",
    " 'loss_function': 'RMSE',\n",
    " 'learning_rate': 0.09000000357627869,\n",
    " 'score_function': 'Cosine',\n",
    " 'task_type': 'CPU',\n",
    " 'leaf_estimation_iterations': 1,\n",
    " 'bootstrap_type': 'MVS',\n",
    " 'max_leaves': 512,\n",
    " 'permutation_count': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CatBoost builds 1000 trees. The number of iterations can be decreased to speed up the training. When the number of iterations decreases, the learning rate needs to be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The smaller the gradient step size, the more iterations you need to train the model. This will increase training time, but can help to more accurately minimize the average error in your loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Use the pandas.Categorical type instead of the object type to speed up the preprocessing for datasets with categorical features up to 200 times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is catboost different from xgboost?<br>\n",
    "Gradient boosting libraries fall under 2 categories:  <br>\n",
    "\n",
    "Those that split at the tree level (XGBoost)  <br>\n",
    "\n",
    "Those that split at the leaf level (LightGBM)  <br>\n",
    "\n",
    "Splitting at the leaf level is faster and can get better accuracy, but requires extensive hyperparameter tuning to not overfit. Catboost solves this problem by combining it with the tree level splits which tend to be more robust to overfitting. As a result, you can get most of the effects of leaf level splits without much of the hyper-parameter tuning. Catboost takes about twice the time to run compared to XGBoost, but it can be debatable which method saves the most time. Here's the Catboost paper if you're interested https://arxiv.org/pdf/1706.09516.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user defined loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catboost.ai/docs/concepts/python-usages-examples.html#user-defined-loss-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/catboost/catboost/issues/112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RMSLE implemented on xgboost](https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Good read custom loss function](https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 005 Final scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  001\n",
    "__Score 2.9__ <br>\n",
    "\n",
    "It was a vanilla model no hyper parameter tuning. 100 iterations. I considered all the variables to work with. <br>\n",
    "{'mae': 10495.339442139302,\n",
    " 'mse': 846828365.6053432,\n",
    " 'rmse': 29100.315558518316,\n",
    " 'r2': -1.7350164169889943}\n",
    " <br>\n",
    " The output was has negative predictions and price cannot go negative. There are suggestions, whenever prices are involved use log to transform. <br>\n",
    " We can see that r2 score is negative indication the model, this means the model is no better than a horizontal line. Also means the model cannot explain the variability in the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 002\n",
    "__Score - 2.4__ <br>\n",
    "From the previous submission I clipped negative to 0 and the socre improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 003\n",
    "__Score - 1.14__<br>\n",
    "1. After the first 2 trials, tested the log tranformation, log transformed the data to make it more gaussian. The accuracy directly improved, and rsme reduced by half. <br>\n",
    "{'mae': 0.7161122603509764,\n",
    " 'mse': 1.2538596551355954,\n",
    " 'rmse': 1.1197587486309697,\n",
    " 'r2': 0.47850914167938985} <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img\\gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trails <br>\n",
    "1. Tried a few combinations of hyper-parameters without gridsearch cv.\n",
    "2. Manually tired to pick a few important and columns to predict, low r2 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 006 Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 001 GridsearchCV parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.estimator: Pass the model instance for which you want to check the hyperparameters. <br>\n",
    "2.params_grid: the dictionary object that holds the hyperparameters you want to try <br>\n",
    "3.scoring: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric <br>\n",
    "4.cv: number of cross-validation you have to try for each selected set of hyperparameters <br>\n",
    "5.verbose: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV <BR>\n",
    "6.n_jobs: number of processes you wish to run in parallel for this task if it -1 it will use all available processors. \n",
    "    \n",
    "\n",
    "\"random_strength\": 0.8}  #We use randomness when scoring the splits. Every split gets a score and then we add some randomness to it, this helps to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_strength\n",
    "This parameter helps to overcome overfitting of the model.\n",
    "\n",
    "When selecting a new split, each possible split gets a score (for example, by how much does adding this split improve the loss function on train). After that all scores are sorted and a split with the highest score is selected.\n",
    "The scores are not random. This parameter adds a normally distributed random variable to the score of the feature. It has zero mean and variance that is larger in the start of the training and decreases during the training. random_strength is the multiplier of the variance.\n",
    "\n",
    "Using randomness during split selection procedure helps to overcome overfitting and increase the resulting quality of the model.\n",
    "\n",
    "bagging_temperature\n",
    "This parameter is responsible for Bayesian bootstrap.\n",
    "Bayesian bootstrap is used by default in classification and regression modes. In ranking we use Bernoulli bootstrap by default.\n",
    "\n",
    "In bayesian bootstrap each object is assigned random weight. If bagging temperature is equal to 1 then weights are sampled from exponential distribution. If bagging temperature is equal to 0 then all weights are equal to 1. By changing this parameter from 0 to +infty you can controll intensity of the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"grow_policy\": \"Lossguide\"  # The tree growing policy. Defines how to perform greedy tree construction.<br>\n",
    "\"min_data_in_leaf\" :  10  # The minimum number of training samples in a leaf. <br> CatBoost does not search for new splits in leaves with samples count less than the specified value.\n",
    "                                # Can be used only with the Lossguide and Depthwise growing policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Do not use one-hot encoding during preprocessing. This affects both the training speed and the resulting quality._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"one_hot_max_size\": 4 # Use one-hot encoding for all categorical features with a number of different values less than or equal to the given parameter value. Ctrs are not calculated for such features. <br>\n",
    "Unlike XGBoost, CatBoost deals with Categorical variables in a native way. Many studies have shown that One-Hot encoding high cardinality categorical features is not the best way to go, especially in tree based algorithms. And other popular alternatives all come under the umbrella of Target Statistics – Target Mean Encoding, Leave-One-Out Encoding, etc.\n",
    "\n",
    "The basic idea of __Target Statistics is simple. We replace a categorical value by the mean of all the targets for the training samples with the same categorical value__. For example, we have a Categorical value called weather, which has four values – sunny, rainy, cloudy, and snow. The most naive method is something called Greedy Target Statistics, where we replace “sunny” with the average of the target value for all the training samples where weather was “sunny”.\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting Detector <br>\n",
    "Another interesting feature in CatBoost is the inbuilt Overfitting Detector. CatBoost can stop training earlier than the number of iterations we set, if it detects overfitting. there are two overfitting detectors implemented in CatBoost –\n",
    "<br>\n",
    "\n",
    "Iter is the equivalent of early stopping where the algorithm waits for n iterations since an improvement in validation loss value before stopping the iterations\n",
    "\n",
    "IncToDec is more slightly involved. It takes a slightly complicated route by keeping track of the improvement of the metric iteration after iteration and also smooths the progression using an approach similar to exponential smoothing and sets a threshold to stop training whenever that smoothed value falls below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"colsample_bylevel\": 0.5, #  The percentage of features to be used in each split selection. This helps us control overfitting and the values range from (0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ordered Boosting](https://deep-and-shallow.com/2020/02/29/the-gradient-boosters-v-catboost/) <br>\n",
    "The main motivation for the CatBoost algorithm is, as argued by the authors of the paper, the target leakage, which they call Prediction Shift, inherent in the traditional Gradient Boosting models. The high-level idea is quite simple. As we know, any Gradient Boosting model works iteratively by building base learners over base learners in an additive fashion. But since each base learner is build based on the same dataset, the authors argue that there is a bit of target leakage which affects the generalization capabilities of the model. Empirically, we know that __Gradient Boosted Trees has an overwhelming tendency to overfit the data. The only countermeasures against this leakage are features like subsampling__, which they argue is a heuristic way of handling the problem and only alleviates it and not completely removes it.\n",
    "\n",
    "The authors formalize the proposed target leakage and mathematically show us that it is present. Another interesting observation that they had is that the target shift or the bias is inversely proportional to the size of the dataset, i.e. if the dataset is small, the target leak is much more pronounced. This observation also agrees with our empirical observation that Gradient Boosted Trees tend to overfit to a small dataset.\n",
    "\n",
    "To combat this issue, they propose a new variant of Gradient Boosting, called Ordered Boosting. The idea, at it’s heart, is quite intuitive. The main problem with previous Gradient Boosting was the reuse of the same dataset for each iteration. So, if we have a different dataset for each of the iteration, we would be solving the problem of leakage. But since none of the datasets are infinite, this idea, purely applied, will not be feasible. So, the authors have proposed a practical implementation of the above concept.\n",
    "\n",
    "It starts out with creating s+1 permutations of the dataset. This permutation is the artificial time that the algorithm takes into account. Let’s call it \\sigma_0\\; to\\; \\sigma_s. The permutations \\sigma_1\\; to\\; \\sigma_s is used for constructing the tree splits and \\sigma_0 is used to choose the leaf values b_j. In the absence of multiple permutations, the training samples with short “history” will have high variance and hence having multiple permutations ease out that defect.\n",
    "\n",
    "We saw the way CatBoost deals with Categorical variables earlier and we mentioned that there we use multiple permutations to calculate the target statistics. This is implemented as part of the boosting algorithm which uses a particular permutation from \\sigma_1\\; to\\; \\sigma_s in any iteration. The gradient statistics required for the tree splits and the target statistics required for the categorical encoding are calculated using the sampled permutation.\n",
    "\n",
    "And once all the trees are built, the leaf values of the final model F are calculated by the standard gradient boosting procedure(that we saw in the previous articles) using permutation \\sigma_0. When the final model F is applied to new examples from test set, the target statistics are calculated on the entire training data.\n",
    "\n",
    "One important thing to note it that CatBoost supports the traditional Gradient Boosting also, apart from the Ordered Boosting (boosting_type = ‘Plain’ or ‘Ordered’). If it is ‘Plain’, and there are categorical features, the permutations are still created for the target statistic, but the tree building and boosting is done without the permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ordered boost wont work for growpolicy logloss, only the default option, also max_depth needs to be 16 or less_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        grid_search_ad = GridSearchCV(  estimator = self._rfr, \n",
    "                                        param_grid = param_grid, \n",
    "                                        scoring = other_dict[\"scoring\"],  ## scoring method\n",
    "                                        cv = other_dict[\"cv\"],    ## no of cross validation\n",
    "                                        n_jobs = -1,    ## no of searches in parallel,-1 means, use all resources\n",
    "                                        verbose = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    params = {'depth':[3,1,2,6,4,5,7,8,9,10],\n",
    "              'iterations':[250,100,500,1000],\n",
    "              'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3],\n",
    "              'l2_leaf_reg':[3,1,5,10,100],\n",
    "              'border_count':[32,5,10,20,50,100,200],\n",
    "              'bagging_temperature':[0.03,0.09,0.25,0.75],\n",
    "              'random_strength':[0.2,0.5,0.8],\n",
    "              'max_ctr_complexity':[1,2,3,4,5] }\n",
    "\n",
    "\n",
    "    model = CatBoostClassifier()\n",
    "    grid_search_result = model.grid_search(params,\n",
    "                                           X=train_set,\n",
    "                                           y=train_label,\n",
    "                                           cv=5,\n",
    "                                           partition_random_seed=3,\n",
    "                                           stratified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 002 \n",
    "There are no null in the train dataset but in the data sent to the model, we could find null, strange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is heavily overfitting data. Need to find a sweet spot, or an technique to find that sweet spot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_forward=SequentialFeatureSelector(estimator=linreg,k_features=(5,10),\n",
    "                                        forward=True,cv=12,\n",
    "                                        scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 004 ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoostError: only one of the parameters depth, max_depth should be initialized.<br>\n",
    "depth and max_depth both cannot be initialised\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoostError: only one of the parameters iterations, n_estimators, num_boost_round, num_trees should be initialized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 005 \n",
    "Early stop models is overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 006 [year a continuous or categorical variable](https://www.researchgate.net/post/Years-as-continuous-variable)\n",
    "Yes. Not only can you use \"year\" as a contiuous variable in your model - you should use it like that!\n",
    "Start with plotting the response by year in a scatterplot. Think of a reasonable functional relationship and a distribution model. In a GLM, the variable \"year\" will be associated with one or more coefficients that should have an interpretable meaning in the functional model. The value of the coefficient and its uncertainty is then estimated from the data, you can perform hypothesis tests / calculate a confidence interval and interpret it.\n",
    "As Mehmet mentioned, the functional relationship could be as simple as linear, in which case the coefficient will be the change (in the response) per year. It is not required here to change the years (to start at 0, as Mehment suggested). It's just that the intercept of the model will refer to the year 0. If you start at year x, the intercept will still refer to the year 0, what will be a practically invalid extrapolation (over x years into the past), but the model is still valid in the range of the years from which you have data. The estimated value of the coefficient for \"year\" (the change per year) won't be affected by using any offset for \"year\" in the model.\n",
    "Offsetting \"year\" may become important for interpretation if you include interactions of \"year\" with other continuous predictors in the model. In such cases I would rather center the variables (subtract the mean form each value, so that the mean of the centered variable is zero).\n",
    "It may be that a linear realationship between the response and year is not very sensible model for your data. If so, the simplest solution is to check if a simple transformation linearizes the relationship. Note that in a GLM the relationship is modelled at the scale of the linear predictor, so you have to take the link function into account (e.g. if you have a binomial model with logit link, the relation ship mut be linear on the logit scale; you should plot the logits of the response against year to check that).\n",
    "GLMM vs GLM: this question is not related to the way how \"year\" is coded in the model. It is related to the correlation structure of your data. If you followed particular subjects, objects or places over time, the data obtained at different years per subject/object/place will be correlated, and this can be accounted for by random effects. A possible alternative to a GLMM may be to fit an individual GLM to each of the n subjects/objects/places so that you get n individual estimates of the relevant coefficient, and you can then further analyse the distribution of these estimates directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 007\n",
    "Cap continuous values to a certain extent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 008 Data cleaning\n",
    "1. Cleaned 2 variables and made them continuous and capped them. The score drastically reduced. \n",
    "        1. convert mileage to continuous \n",
    "        2. replace 0 mileage with average mileage of a year- based on EDA. \n",
    "        3. Capping upper limit based on \n",
    "        (df_price_train[(df_price_train[\"Mileage\"]<=400000) & (df_price_train[\"Mileage\"] > 0)].Mileage).hist(bins = 100)\n",
    "        4. remove less than equal to 0 mileage\n",
    "2. \n",
    "        1. split engine volume and make engine volume columns float. \n",
    "        2. in the new column turbo_flag_n, replace None with cat. \n",
    "        3. Remove outliers capped at 0 and 10 lower and upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 009 \n",
    "CatBoostError: Invalid cat_features[1] = 2 value: index must be < 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 010 \n",
    "[Plot tree](https://catboost.ai/docs/concepts/python-reference_catboost_plot_tree.html)<br>\n",
    "[get_borders](https://github.com/catboost/catboost/issues/1569)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"cat_features\": [], \n",
    "          \"loss_function\": \"RMSE\", \n",
    "          \"od_type\":\"IncToDec\",  # overfitting detector.\n",
    "          \"random_seed\" : 42,\n",
    "          \"colsample_bylevel\": 0.5, #  The percentage of features to be used in each split selection. This helps us control overfitting and the values range from (0,1].\n",
    "          # \"iterations\" : 100, \n",
    "          \"verbose\":0,\\\n",
    "          'learning_rate': 0.1,\n",
    "          # \"boosting_type\" : \"Ordered\" , # to counter leakage when sample size is small\n",
    "          ## this one doesnt give good results\n",
    "          ## This takes time, idk \n",
    "          #'depth': 8,   ##depth of the trees\n",
    "          'l2_leaf_reg': 20, \n",
    "          \"max_depth\" : 16,  #max depth /depth of 10 makes sense \n",
    "          \"model_size_reg\" : 5,\n",
    "          \"n_estimators\": 1000,\n",
    "          \"random_strength\": 0.4,  #We use randomness when scoring the splits. Every split gets a score and then we add some randomness to it, this helps to reduce overfitting.\n",
    "          # \"bootstrap_type \" :\"Bayesian\",\n",
    "#           \"bagging_temperature\": 2,    # 0 to +infty  / Only with Bayesian bootstraping\n",
    "          \"eval_metric\" : \"MSLE\" , #The metric used for overfitting detection \n",
    "           # \"grow_policy\": \"Lossguide\" , # The tree growing policy. Defines how to perform greedy tree construction.\n",
    "          \"min_data_in_leaf\" :  10,  # The minimum number of training samples in a leaf. CatBoost does not search for new splits in leaves with samples count less than the specified value.\n",
    "                              # Can be used only with the Lossguide and Depthwise growing policies.\n",
    "#           \"one_hot_max_size\": 4, # Use one-hot encoding for all categorical features with a number of different values less than or equal to the given parameter value. Ctrs are not calculated for such features.\n",
    "          \"score_function\":\"L2\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. with n_estiamtors - 1000 trains slows, if \"od_type\":\"IncToDec\", is removed trains fast. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. If learning rate is low then we need more iterations to converge. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. score_function\":\"L2\"  # The score type used to select the next split during the tree construction.  <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. iterations / n_estimators tend to overfit, need to counter with some other parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Loss function - The model tries to optimize by reducing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. \"od_type\":\"IncToDec\", \"od_pval\" : 30 go hand in hand.   do not use with [iter type of od_type](https://catboost.ai/docs/concepts/parameter-tuning.html#trees-number__overfitting-detection-settings) or <br>\n",
    "od_wait and od_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Tree depth increase modelling training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
